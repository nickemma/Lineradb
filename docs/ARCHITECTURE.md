# LineraDB Architecture

**Status:** Early Development  
**Last Updated:** December 2025  
**Author:** [@Nicholas Emmanuel](https://github.com/nickemma)

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Design Principles](#design-principles)
- [System Architecture](#system-architecture)
- [Module Breakdown](#module-breakdown)
- [Data Flow](#data-flow)
- [Network Architecture](#network-architecture)
- [Failure Handling](#failure-handling)
- [Future Evolution](#future-evolution)

---

## Overview

LineraDB is a **distributed SQL database** built from first principles to demonstrate mastery of distributed systems concepts. The architecture is designed to be:

- **Learnable** - Clear module boundaries with explicit dependencies
- **Evolvable** - Incremental complexity (single-node â†’ multi-region)
- **Testable** - Hexagonal architecture enables unit testing without infrastructure
- **Realistic** - Production-grade patterns from Google Spanner, CockroachDB

**Core Philosophy:** Build the simplest thing that could work, then evolve complexity as needed.

---

## Design Principles

### 1. Hexagonal Architecture (Ports & Adapters)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Domain Layer (Pure Logic)       â”‚
â”‚  - No external dependencies             â”‚
â”‚  - Business rules & entities            â”‚
â”‚  - Testable without mocks               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Application Layer (Use Cases)      â”‚
â”‚  - Orchestrates domain logic            â”‚
â”‚  - Depends on repository interfaces     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Repository Layer (Port Interfaces)   â”‚
â”‚  - Contracts for external systems       â”‚
â”‚  - Storage, network, consensus          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Infrastructure Layer (Adapters)       â”‚
â”‚  - Concrete implementations             â”‚
â”‚  - gRPC, RocksDB, Raft library          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why?** This allows replacing implementations (e.g., in-memory storage â†’ persistent storage) without touching business logic.

### 2. Domain-Driven Design (DDD)

Each module represents a **bounded context** with:

- **Ubiquitous Language** - Terminology matches domain experts (Raft, MVCC, 2PC)
- **Aggregates** - Transactional consistency boundaries
- **Entities & Value Objects** - Clear identity semantics

### 3. Contract-First Development

- **Protobuf IDL** defines all inter-module APIs
- Generates Go/Rust code for type safety
- Enables language-agnostic evolution

### 4. Explicit Constraints

All physical/logical constraints are documented (see [CONSTRAINTS.md](CONSTRAINTS.md)):

- Network latency (speed of light)
- CAP theorem trade-offs
- Failure modes (crash, partition, Byzantine)

---

## System Architecture

### High-Level Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Client Applications                      â”‚
â”‚              (SQL drivers, REST API, CLI)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ SQL/gRPC
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   SQL Query Layer (Go)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Parser  â”‚â†’ â”‚ Planner  â”‚â†’ â”‚Optimizer â”‚â†’ â”‚ Executor â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Transaction Coordinator (Go)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚     2PC      â”‚  â”‚     MVCC     â”‚  â”‚   Snapshot   â”‚    â”‚
â”‚  â”‚  Coordinator â”‚  â”‚   Timestamp  â”‚  â”‚   Isolation  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Distributed Layer (Go)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚    Raft      â”‚  â”‚   Sharding   â”‚  â”‚ Replication  â”‚    â”‚
â”‚  â”‚  Consensus   â”‚  â”‚  (Consistent â”‚  â”‚ (Cross-      â”‚    â”‚
â”‚  â”‚  (Leader     â”‚  â”‚   Hashing)   â”‚  â”‚  Region)     â”‚    â”‚
â”‚  â”‚   Election)  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Storage Engine (Rust + Go FFI)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   LSM Tree   â”‚  â”‚     WAL      â”‚  â”‚  Compaction  â”‚    â”‚
â”‚  â”‚  (Memtable,  â”‚  â”‚  (Durability)â”‚  â”‚  (Background â”‚    â”‚
â”‚  â”‚   SSTables)  â”‚  â”‚              â”‚  â”‚   Thread)    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Persistent Storage                       â”‚
â”‚                  (Local Disk, Cloud Block)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Layer Responsibilities

| Layer                       | Language | Responsibility                         | Key Algorithms                          |
| --------------------------- | -------- | -------------------------------------- | --------------------------------------- |
| **SQL Layer**               | Go       | Parse, plan, optimize, execute queries | Query planning, cost-based optimization |
| **Transaction Coordinator** | Go       | Ensure ACID properties                 | 2PC, MVCC, snapshot isolation           |
| **Distributed Layer**       | Go       | Consensus, replication, sharding       | Raft, consistent hashing                |
| **Storage Engine**          | Rust     | Persistent storage, indexing           | LSM trees, compaction                   |

---

## Module Breakdown

### Current Modules (Phase 1)

#### 1. `internal/clock` - Hybrid Logical Clock

**Purpose:** Provide causal ordering across distributed nodes without synchronized physical clocks.

```
internal/clock/
â”œâ”€â”€ domain/
â”‚   â””â”€â”€ hlc.go              # HLC entity (timestamp + logical counter)
â”œâ”€â”€ application/
â”‚   â””â”€â”€ clock_service.go    # Use cases (generate, update, compare)
â”œâ”€â”€ repository/
â”‚   â””â”€â”€ clock_repo.go       # Interface for clock persistence
â””â”€â”€ infrastructure/
    â””â”€â”€ memory_clock.go     # In-memory implementation
```

**Key Concepts:**

- **Physical Time:** Wall-clock time (may drift)
- **Logical Counter:** Disambiguates events with same physical time
- **Happens-Before:** `A â†’ B` if `HLC(A) < HLC(B)`

**Implementation:**

```go
type HLC struct {
    PhysicalTime int64  // Nanoseconds since epoch
    LogicalTime  int64  // Monotonic counter
}

func (c *HLC) Update(remote *HLC) {
    c.PhysicalTime = max(c.PhysicalTime, remote.PhysicalTime, wallClock())
    if c.PhysicalTime == remote.PhysicalTime {
        c.LogicalTime = max(c.LogicalTime, remote.LogicalTime) + 1
    } else {
        c.LogicalTime = 0
    }
}
```

**Trade-offs:** See [TRADEOFFS.md](TRADEOFFS.md#hybrid-logical-clock)

---

#### 2. `internal/consensus` - Raft Consensus (Planned)

**Purpose:** Ensure replicated state machines agree on log order despite failures.

```
internal/consensus/
â”œâ”€â”€ domain/
â”‚   â”œâ”€â”€ log_entry.go        # Log entries (commands + metadata)
â”‚   â”œâ”€â”€ state.go            # Raft state (Leader/Follower/Candidate)
â”‚   â””â”€â”€ term.go             # Election terms
â”œâ”€â”€ application/
â”‚   â”œâ”€â”€ raft_service.go     # Raft state machine
â”‚   â”œâ”€â”€ election.go         # Leader election logic
â”‚   â””â”€â”€ replication.go      # Log replication
â”œâ”€â”€ repository/
â”‚   â”œâ”€â”€ log_repo.go         # Persistent log interface
â”‚   â””â”€â”€ state_repo.go       # Persistent state (term, votedFor)
â””â”€â”€ infrastructure/
    â”œâ”€â”€ grpc_transport.go   # gRPC for RPC calls
    â””â”€â”€ disk_log.go         # Disk-backed log
```

**Raft Core Algorithms:**

1. **Leader Election:**

   - Nodes start as Followers
   - If no heartbeat â†’ Candidate â†’ requests votes
   - Majority votes â†’ becomes Leader

2. **Log Replication:**

   - Leader appends entries to local log
   - Sends `AppendEntries` RPC to Followers
   - Commits entry when majority acknowledges

3. **Safety:**
   - **Election Safety:** At most one leader per term
   - **Log Matching:** If two logs contain entry with same index/term, all preceding entries match
   - **Leader Completeness:** If entry committed in term T, it appears in logs of all leaders â‰¥ T

**Why Raft?** Simpler than Paxos, proven correctness, widely used (etcd, CockroachDB).

---

#### 3. `internal/storage` - Storage Engine (Planned)

**Purpose:** Persistent, crash-safe storage with efficient reads/writes.

```
internal/storage/
â”œâ”€â”€ domain/
â”‚   â”œâ”€â”€ key_value.go        # Key-value pair entity
â”‚   â””â”€â”€ memtable.go         # In-memory sorted map
â”œâ”€â”€ application/
â”‚   â”œâ”€â”€ lsm_service.go      # LSM tree operations
â”‚   â””â”€â”€ compaction.go       # Background compaction
â”œâ”€â”€ repository/
â”‚   â””â”€â”€ storage_repo.go     # Storage interface
â””â”€â”€ infrastructure/
    â”œâ”€â”€ sstable.go          # Sorted String Table (disk format)
    â”œâ”€â”€ wal.go              # Write-Ahead Log
    â””â”€â”€ bloom_filter.go     # Probabilistic membership test
```

**LSM Tree Structure:**

```
Write Path:
1. Append to WAL (durability)
2. Insert into Memtable (in-memory)
3. When Memtable full â†’ flush to SSTable (disk)

Read Path:
1. Check Memtable
2. Check Bloom filters (skip SSTables without key)
3. Search SSTables (newest to oldest)

Compaction:
- Merge SSTables to remove deleted keys
- Reduce read amplification
```

**Why LSM?** Write-optimized (sequential writes), compaction amortizes cost, used by RocksDB/LevelDB.

---

### Future Modules (Phase 2+)

#### 4. `internal/sql` - SQL Query Layer

**Components:**

- **Parser:** SQL â†’ AST (abstract syntax tree)
- **Planner:** AST â†’ logical plan (relational algebra)
- **Optimizer:** Logical plan â†’ physical plan (cost-based)
- **Executor:** Physical plan â†’ results (iterator model)

#### 5. `internal/transaction` - Transaction Coordinator

**Algorithms:**

- **Two-Phase Commit (2PC):** Atomic commit across shards
- **MVCC:** Multiple versions of data for snapshot reads
- **Snapshot Isolation:** Transactions see consistent snapshot

#### 6. `internal/sharding` - Data Partitioning

**Strategies:**

- **Consistent Hashing:** Minimize reshuffling during rebalancing
- **Range Partitioning:** Co-locate related keys
- **Metadata Service:** Track shard â†’ node mapping

#### 7. `internal/replication` - Cross-Region Replication

**Patterns:**

- **Follower Reads:** Read from nearest replica (eventual consistency)
- **Leader Leases:** Time-bound leadership for linearizable reads
- **Conflict Resolution:** Last-write-wins, CRDTs

---

## Data Flow

### Write Path (Single-Node, Phase 1)

```
Client â†’ SQL Query â†’ Parser â†’ Planner â†’ Executor
                                              â†“
                                     Transaction Begin
                                              â†“
                                       Storage Engine
                                              â†“
                                     WAL (flush to disk)
                                              â†“
                                     Memtable (in-memory)
                                              â†“
                                     Transaction Commit
                                              â†“
                                       Return to Client
```

### Write Path (Multi-Node, Phase 2)

```
Client â†’ SQL Query â†’ Leader Node
                          â†“
                   Transaction Coordinator
                          â†“
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â†“                     â†“
         Shard A (Raft)        Shard B (Raft)
               â†“                     â†“
           Prepare?              Prepare?
               â†“                     â†“
           Yes (vote)            Yes (vote)
               â†“                     â†“
         Commit (2PC)          Commit (2PC)
               â†“                     â†“
        Storage Engine        Storage Engine
```

### Read Path (Linearizable)

```
Client â†’ SQL Query â†’ Leader Node (has lease)
                          â†“
                   Check MVCC Timestamp
                          â†“
                   Storage Engine (read)
                          â†“
                   Return to Client
```

**Trade-off:** Linearizable reads require leader, adding latency. See [TRADEOFFS.md](TRADEOFFS.md#read-consistency).

---

## Network Architecture

### Single-Region (Phase 1-3)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Availability Zone 1            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚Node 1â”‚  â”‚Node 2â”‚  â”‚Node 3â”‚          â”‚
â”‚  â”‚(Ldr) â”‚  â”‚(Flwr)â”‚  â”‚(Flwr)â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚      â†•          â†•          â†•            â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚       gRPC (Raft heartbeats)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Multi-Region (Phase 5)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   us-west-2     â”‚  â”‚   us-east-1     â”‚  â”‚   eu-west-1     â”‚
â”‚  â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”  â”‚
â”‚  â”‚N1  â”‚ â”‚N2  â”‚  â”‚  â”‚  â”‚N3  â”‚ â”‚N4  â”‚  â”‚  â”‚  â”‚N5  â”‚ â”‚N6  â”‚  â”‚
â”‚  â”‚Ldr â”‚ â”‚Flwrâ”‚  â”‚  â”‚  â”‚Flwrâ”‚ â”‚Flwrâ”‚  â”‚  â”‚  â”‚Flwrâ”‚ â”‚Flwrâ”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                    â”‚                    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              WAN (VPC peering, ~50-150ms RTT)
```

**Challenges:**

- **Latency:** Cross-region Raft quorum requires WAN roundtrip (>100ms)
- **Partitions:** Split-brain prevention requires majority quorum
- **Clock Skew:** HLC handles unsynchronized clocks

---

## Failure Handling

### Failure Taxonomy

| Failure Type          | Detection                          | Recovery                    | Example         |
| --------------------- | ---------------------------------- | --------------------------- | --------------- |
| **Crash-Stop**        | Heartbeat timeout                  | Raft leader election        | Node OOM kill   |
| **Network Partition** | RPC timeout                        | Quorum-based decisions      | AWS AZ outage   |
| **Byzantine**         | Not handled (trust infrastructure) | N/A                         | Malicious node  |
| **Clock Skew**        | HLC comparison                     | Reject if drift > threshold | NTP failure     |
| **Disk Failure**      | I/O errors                         | Replicate to healthy node   | Disk corruption |

### Raft Safety During Failures

**Scenario 1: Leader Crashes**

```
Before:  Leader (N1) â†’ Followers (N2, N3)
After:   N1 crashes
Result:  N2 or N3 elected (majority still available)
Time:    ~election_timeout (150-300ms)
```

**Scenario 2: Network Partition**

```
Before:  3 nodes (N1, N2, N3) in same DC
After:   Partition isolates N1 | N2, N3
Result:  N2 or N3 becomes leader (majority in partition)
         N1 steps down (cannot reach quorum)
Safety:  Old leader cannot commit (no quorum)
```

**Scenario 3: Split-Brain Prevention**

```
Before:  Old leader (N1, term=5) isolated
         New leader (N2, term=6) elected
After:   Network heals, N1 rejoins
Result:  N1 sees higher term â†’ steps down
         N1 replicates N2's log
Safety:  Term numbers prevent dual leadership
```

---

## Future Evolution

### Phase 1 â†’ Phase 2: Single-Node â†’ Replicated

**Changes:**

- Add Raft module
- Replace in-memory storage â†’ persistent storage (LSM)
- Add gRPC transport for inter-node communication

**No Changes:**

- SQL parser/planner (still operates on single node)
- Transaction semantics (still single-node transactions)

### Phase 2 â†’ Phase 3: Replicated â†’ Distributed Transactions

**Changes:**

- Add transaction coordinator (2PC)
- Implement MVCC in storage engine
- Add distributed query executor

**No Changes:**

- Raft consensus (same algorithm)
- Storage engine (same LSM)

### Phase 3 â†’ Phase 4: Single-Partition â†’ Sharded

**Changes:**

- Add sharding module (consistent hashing)
- Add metadata service (shard placement)
- Modify query planner (cross-shard execution)

### Phase 4 â†’ Phase 5: Single-Region â†’ Multi-Region

**Changes:**

- Deploy Raft clusters across regions
- Add leader leases for linearizable reads
- Implement conflict resolution

---

## ğŸ“š References

- **Raft Paper:** [In Search of an Understandable Consensus Algorithm](https://raft.github.io/raft.pdf)
- **LSM Trees:** [The Log-Structured Merge-Tree (LSM-Tree)](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.2782)
- **Spanner:** [Spanner: Google's Globally Distributed Database](https://research.google/pubs/pub39966/)
- **CockroachDB:** [Architecture Overview](https://www.cockroachlabs.com/docs/stable/architecture/overview.html)
- **HLC:** [Logical Physical Clocks](https://cse.buffalo.edu/tech-reports/2014-04.pdf)

---

## ğŸ¤ Contributing

Architecture decisions are documented in ADRs (Architecture Decision Records) in `docs/adr/`. Before proposing changes, please:

1. Read existing ADRs
2. Understand current constraints (see [CONSTRAINTS.md](CONSTRAINTS.md))
3. Consider trade-offs (see [TRADEOFFS.md](TRADEOFFS.md))
4. Open an issue for discussion

---

<div align="center">

**Built with â¤ï¸ by [@Nicholas Emmanuel](https://github.com/nickemma)**

[â¬† Back to Top](#lineradb-architecture)

</div>
